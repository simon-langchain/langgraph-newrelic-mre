"""
Minimal LangGraph Agent Example

A simple LangGraph agent that invokes an LLM with OpenTelemetry tracing to New Relic.
"""

import asyncio
import os
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI

# Initialize OpenTelemetry tracing to New Relic
def setup_otel_tracing():
    """
    Configure OpenTelemetry to send traces to New Relic via OTLP endpoint.
    
    Environment variables required:
    - OTEL_EXPORTER_OTLP_ENDPOINT: New Relic OTLP endpoint (default: https://otlp.nr-data.net)
    - OTEL_EXPORTER_OTLP_HEADERS: API key header (format: "api-key=<license_key>")
    - OTEL_SERVICE_NAME: Service name for tracing (optional)
    """
    try:
        from opentelemetry import trace
        from opentelemetry.sdk.trace import TracerProvider
        from opentelemetry.sdk.trace.export import BatchSpanProcessor
        from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
        
        # Set default environment variables for New Relic if not already configured
        if not os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT"):
            os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "https://otlp.nr-data.net"
        
        if not os.getenv("OTEL_SERVICE_NAME"):
            os.environ["OTEL_SERVICE_NAME"] = "langgraph-newrelic-mre"
        
        # Only initialize if OTLP headers (API key) are configured
        if os.getenv("OTEL_EXPORTER_OTLP_HEADERS"):
            # Create OTLP exporter
            otlp_exporter = OTLPSpanExporter(
                timeout=10,
            )
            
            # Set up tracer provider with batch span processor
            tracer_provider = TracerProvider()
            tracer_provider.add_span_processor(
                BatchSpanProcessor(otlp_exporter)
            )
            
            # Set as global tracer provider
            trace.set_tracer_provider(tracer_provider)
            
            print("‚úÖ OpenTelemetry tracing to New Relic initialized")
            return True
        else:
            print("‚ÑπÔ∏è OTEL_EXPORTER_OTLP_HEADERS not set - OTEL tracing disabled")
            return False
    except ImportError:
        print("‚ö†Ô∏è OpenTelemetry packages not installed - tracing disabled")
        return False
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to initialize OTEL tracing: {e}")
        return False

# Initialize tracing on module load
setup_otel_tracing()


class State(TypedDict):
    """Simple state for our agent."""
    messages: Annotated[list, add_messages]


def chatbot(state: State):
    """
    Simple chatbot node that calls an LLM.
    Traces execution to OpenTelemetry/New Relic.
    """
    messages = state["messages"]
    
    # Get tracer for this module
    tracer = None
    try:
        from opentelemetry import trace
        tracer = trace.get_tracer(__name__)
    except ImportError:
        pass
    
    # Create a span for this operation
    span_context = tracer.start_as_current_span("chatbot_invoke") if tracer else None
    
    try:
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        response = llm.invoke(messages)
        
        if tracer and span_context:
            span_context.__enter__().set_attribute("llm.response.success", True)
        
        return {"messages": [response]}
    except Exception as e:
        if tracer and span_context:
            span_context.__enter__().set_attribute("llm.response.error", str(e))
        
        print(f"‚ö†Ô∏è LLM error: {e}")
        # Echo mode fallback
        last_message = messages[-1]
        echo_response = {
            "role": "assistant",
            "content": f"Echo: {last_message.content if hasattr(last_message, 'content') else str(last_message)}"
        }
        return {"messages": [echo_response]}
    finally:
        if span_context:
            span_context.__exit__(None, None, None)


# Build the graph
print("üî® Building LangGraph...")
graph_builder = StateGraph(State)
graph_builder.add_node("chatbot", chatbot)
graph_builder.add_edge(START, "chatbot")
graph_builder.add_edge("chatbot", END)

# Compile the graph
async def compile_graph():
    def _compile():
        return graph_builder.compile()
    return await asyncio.to_thread(_compile)

graph = asyncio.run(compile_graph())

print("‚úÖ LangGraph compiled successfully")
print("üöÄ Ready to deploy!")

# This is what LangSmith/LangGraph Platform will import
__all__ = ["graph"]

